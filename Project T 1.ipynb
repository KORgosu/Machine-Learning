{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:20:55.114151Z","iopub.status.busy":"2023-06-02T18:20:55.113788Z","iopub.status.idle":"2023-06-02T18:21:06.369396Z","shell.execute_reply":"2023-06-02T18:21:06.368367Z","shell.execute_reply.started":"2023-06-02T18:20:55.114121Z"},"papermill":{"duration":13.875729,"end_time":"2023-05-08T06:10:55.254188","exception":false,"start_time":"2023-05-08T06:10:41.378459","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from konlpy) (4.9.2)\n","Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.10/site-packages (from konlpy) (1.23.5)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install konlpy  # 토큰화에 사용할 konlpy 라이브러리 설치"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","[프로젝트-1] 영화 리뷰 분석하기\n","데이터셋 설명\n","본 데이터셋은 네이버 영화 서비스에 사용자들이 남긴 리뷰들로 구성되었습니다.\n","데이터셋에는 길이 제한이 없는 리뷰글과, 해당 리뷰가 영화에 대한 긍정적 리뷰인지, 혹은 부정적 리뷰인지의 여부가 포함되어 있습니다.\n","자연어(Natrual Language) 데이터, 그 중에서도 우리가 사용하는 한국어 데이터를 전처리하여 텍스트 데이터를 머신러닝을 적용하는 과정을 통해, 데이터 전처리와 특징 추출의 과정을 배워보시길 바랍니다.\n","자연어 처리 (Natural Language Processing, NLP)\n","자연어 데이터를 머신러닝에 사용하기 위해서는 데이터를 머신러닝에 사용할 수 있도록 전처리하는 과정이 필요합니다.\n","일반적으로 머신러닝에 사용되는 데이터가 어떤 형태이고, 자연어가 이와 어떻게 다른지 생각해봅시다.\n","데이터의 크기: 대부분의 머신러닝 모델들은 고정된 크기의 입력 데이터를 받습니다. 그러나, 자연어는 문장에 따라 길이가 상이합니다. 때문에 자연어를 머신러닝 모델에 투입하려면 데이터를 고정된 크기로 변환해줘야 합니다.\n","데이터의 형태: 머신러닝 모델들은 실수 데이터를 입력 받습니다. 그러나, 자연어 데이터는 문자형(char, string)으로 되어있습니다.\n","이러한 문제들로 인해 자연어 처리에는 전처리의 방식이 매우 중요합니다.\n","이 과제에서, 자연어 데이터의 전처리를 경험해보고 대표적인 두 가지 방법론의 성능을 비교해봅시다.\n","\n","[Empty Module #1]\n","자연어 데이터의 전처리\n","\n","먼저, 정규표현식을 이용해 한글(초성 제외), 영문자, 띄어쓰기를 제외한 문자들을 제외해봅시다.\n","초성체(ㅋㅋ, ㅋㅋㅋ)나 특수문자(. , ? ! !! 등)와 같이 처리하기 어렵고 문장의 의미에 큰 영향을 주지 않는 데이터들을 제거합니다.\n","영어의 경우, 같은 단어들이 대소문자에 의해 다른 단어로 인식되지 않도록 모두 대문자로 통일해줍니다.\n","(apple, APPLE, aPPle 등을 모두 APPLE로 통일합니다.)\n","\n","[Empty Module #2]\n","단어 토큰화(Tokenization)\n","\n","이때, 어근화(stemming)를 적용합니다. 어근화란 같은 뜻을 갖지만 다양한 형태로 표현되는 단어들을 한 가지 형태로 통일 시켜주는 작업입니다.\n","예를 들어, \"~입니다\", \"~에요\", \"~임\" 등을 \"이다\"로 통일하는 작업입니다.\n","konlpy 라이브러리에서 제공하는 한국어 토크나이저인 okt를 활용하여 단어 토큰화를 수행합니다.\n","\n","[Empty Module #3]\n","불용어 제거\n","\n","[\"은\", \"는\", \"이\", \"가\"]와 같은 단어들은 그 자체로는 큰 의미를 가지지 않는 불용어(stopword)입니다.\n","이러한 데이터들 역시 기계학습 모델이 쉽게 이해하게 하기 매우 어려우며, 본 task에서 문장의 내용을 판별하는데 크게 도움이 되지도 않습니다.\n","\n","[Empty Module #4]\n","단어 임베딩(Word Embedding)\n","\n","토큰화된 단어를 컴퓨터가 이해할 수 있는 실수 벡터 형태로 변환하는 과정을 단어 임베딩이라고 합니다.\n","이러한 임베딩에도 매우 다양한 방법이 존재하며, 각각의 장점과 단점이 존재합니다.\n","-심지어 <한국어 임베딩>이라고 하는 유명한 책이 있을 정도로, 임베딩은 굉장히 깊은 분야입니다.\n","이번 프로젝트에서는 가장 단순한 방법이라 할 수 있는 희소 표현법으로 단어를 임베딩 해보겠습니다.\n","희소 표현법(Sparse Representation)\n","희소 표현법은 각 단어에 하나의 값들을 부여합니다.\n","예를들어, \"사과\"에 1, \"맛있다.\"에 2… 와 같은 방식입니다. (데이터 전처리에서 배운 라벨 인코딩과 유사합니다.)\n","이 방법을 사용하면 데이터에 등장하는 단어의 수가 m개라 할 때, (0~m) 사이의 값으로 각 단어들이 임베딩됩니다.\n","이를 원-핫 인코딩 방식으로 표현하면 [0, 0, 1, 0 ..... 0]과 같이 대부분의 값이 0이고 하나의 값만 1인, 희소 벡터로 변환되기 때문에 이러한 방식을 희소 표현법이라 합니다.\n","(참고) 한국어 자연어 처리의 어려움\n","한국어는 영어나 라틴어 계열의 다른 언어에 비해 토큰화를 수행하기 어려운 언어에 속합니다.\n","영어에서는 \"San Francisco\", \"We're\"와 같이 일부 예외를 제외하고는 띄어쓰기를 활용하면 쉽게 단어 토큰화를 수행할 수 있습니다.\n","한편, 한국어는 조사, 어미와 같이 다른 단어와 결합되는 단어들을 통해 말이 구성되는 교착어에 속합니다.\n","예를 들어, 영어에서는 \"I\"로 표현되는 말이 한국어에서는 \"나는\", \"내가\" 등으로 다양하게 나타날 수 있기 때문에, 띄어쓰기를 통한 분리가 어렵습니다.\n","\n","[Empty Module #5]\n","Bag of Words 방법을 통한 문장 벡터화\n","위에서 한 개의 단어를 한 개의 값으로 변환하였습니다.\n","그러나 여전히 우리가 분류해야 할 문장들은 다양한 길이를 가지고 있기 때문에, 고정된 크기의 입력 값을 요구하는 머신러닝 모델을 적용하기 어렵습니다.\n","Bag of Words(BoW) 방법을 사용하여 모든 문장을 동일한 길이의 벡터로 만들어 줄 수 있습니다.\n","이렇게 변환된 벡터의 형태를 Bag of Words(BoW)라고 합니다.\n","Bag of Words 방법\n","BoW는 그 이름처럼, 어떤 단어들이 담긴 가방(집합)의 형태로 문장을 표현한다고 이해할 수 있습니다.\n","마치 가방 속에 든 물건의 개수를 세는 것처럼, 어떠한 문장을 고정된 $m$개의 단어들의 등장 횟수로 나타내는 방법입니다.\n","예를들어, \"사과 주스는 사과 맛이 나는 주스입니다.\"와 같은 문장은 {\"사과\": 2, \"주스\": 2, \"는\":2, \"맛\": 1, \"이\":1, \"나\":1\", \"입니다.\":1, \"당근\":0, \"인공지능\": 0 ....}과 같이 나타낼 수 있습니다.\n","임베딩된 자연어 데이터를 BoW 형태로 변환하는 코드를 직접 작성하여, 데이터를 변환해봅시다.\n","\n","[Empty Module #6]\n","차원 축소\n","전체 데이터에서 50번 미만으로 등장하는 단어들을 BoW에서 제거하는 코드를 작성해봅시다.\n","\n","[Empty Module #7]\n","분류 수행 및 제출: BoW\n","\n","[Empty Module #8]\n","TF-IDF 방법\n","아래 TF-IDF 방법에 대한 설명을 읽고, 직접 구현\n","\n","TF-IDF(Term Frequency - Inverse Document Frequency) 방법은 BoW 방법에서 조금 더 나아가, 각 단어들의 중요도를 고려합니다.\n","먼저, 각 단어 t가 전체 데이터 N개 중 몇 개의 샘플(문서)에서 등장하는지를 의미하는 document frequency를 구합니다.\n","아래 식을 참고하여 앞서 구한 document frequency에 반비례하는 inverse document frequency를 구합니다.\n","BoW Feature에 IDF를 곱하여 중요도를 반영해줍니다.\n","\n","[Empty Module #9]\n","분류 수행 및 제출: TF-IDF\n","TF-IDF의 적용 여부를 제외하고 전처리 과정, 모델 구조 등에는 변화를 주지 않아야 정확한 비교가 가능합니다.\n","이렇게 다른 조건들은 동일한 상태로, 어떤 변화가 성능에 미치는 영향을 알아보기 위한 실험을 ablation study라고 합니다.\n","'''"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:21:06.371436Z","iopub.status.busy":"2023-06-02T18:21:06.371131Z","iopub.status.idle":"2023-06-02T18:21:06.377868Z","shell.execute_reply":"2023-06-02T18:21:06.376711Z","shell.execute_reply.started":"2023-06-02T18:21:06.371409Z"},"papermill":{"duration":0.022219,"end_time":"2023-05-08T06:10:55.287013","exception":false,"start_time":"2023-05-08T06:10:55.264794","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os, random\n","from tqdm import tqdm # 진행도 시각화를 위한 라이브러리\n","\n","seed=42\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","random.seed(seed)\n","np.random.seed(seed)"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.009253,"end_time":"2023-05-08T06:10:55.306186","exception":false,"start_time":"2023-05-08T06:10:55.296933","status":"completed"},"tags":[]},"source":["# 데이터 불러오기\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:21:06.379605Z","iopub.status.busy":"2023-06-02T18:21:06.379324Z","iopub.status.idle":"2023-06-02T18:21:07.128408Z","shell.execute_reply":"2023-06-02T18:21:07.127492Z","shell.execute_reply.started":"2023-06-02T18:21:06.379581Z"},"papermill":{"duration":0.863577,"end_time":"2023-05-08T06:10:56.179319","exception":false,"start_time":"2023-05-08T06:10:55.315742","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(149993, 2)\n","(49999, 1)\n"]}],"source":["train_data = pd.read_csv(\"/kaggle/input/2023-ml-project1/nsmc_train.csv\", index_col=0)\n","test_data = pd.read_csv(\"/kaggle/input/2023-ml-project1/nsmc_test.csv\", index_col=0)\n","print(train_data.shape)\n","print(test_data.shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:21:07.131307Z","iopub.status.busy":"2023-06-02T18:21:07.130314Z","iopub.status.idle":"2023-06-02T18:21:07.157147Z","shell.execute_reply":"2023-06-02T18:21:07.156165Z","shell.execute_reply.started":"2023-06-02T18:21:07.131279Z"},"papermill":{"duration":0.044922,"end_time":"2023-05-08T06:10:56.234099","exception":false,"start_time":"2023-05-08T06:10:56.189177","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>rating</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9324809</th>\n","      <td>배우들의 인생연기가 돋보였던... 최고의 드라마</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9305425</th>\n","      <td>아 혜리 보고싶다 ... 여군좀 ㅠ</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5239110</th>\n","      <td>눈이 팅팅..... 정말 ,..... 대박이다......</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9148159</th>\n","      <td>캐슬린 터너의 보디는 볼만했다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6144938</th>\n","      <td>진짜 최고였다.</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                  review  rating\n","id                                              \n","9324809       배우들의 인생연기가 돋보였던... 최고의 드라마       1\n","9305425              아 혜리 보고싶다 ... 여군좀 ㅠ       0\n","5239110  눈이 팅팅..... 정말 ,..... 대박이다......       1\n","9148159                 캐슬린 터너의 보디는 볼만했다       0\n","6144938                         진짜 최고였다.       1"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_data.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:21:07.158999Z","iopub.status.busy":"2023-06-02T18:21:07.158578Z","iopub.status.idle":"2023-06-02T18:21:07.162992Z","shell.execute_reply":"2023-06-02T18:21:07.162437Z","shell.execute_reply.started":"2023-06-02T18:21:07.158971Z"},"papermill":{"duration":0.032904,"end_time":"2023-05-08T06:10:56.282527","exception":false,"start_time":"2023-05-08T06:10:56.249623","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["x_train = train_data[\"review\"]\n","y_train = np.array(train_data[\"rating\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.01003,"end_time":"2023-05-08T06:10:56.302834","exception":false,"start_time":"2023-05-08T06:10:56.292804","status":"completed"},"tags":[]},"source":["# 자연어 전처리\n","## \\[Empty Module #1\\] 데이터 전처리\n","### 데이터 전처리 수행\n","> 먼저, 리뷰를 분류하는데 도움이 되거나, 머신러닝 처리에 어려운 단어들을 제거해봅시다.\n","1. 아래 조건에 맞는 정규표현식을 작성하여 영어와 한글 문자를 제외한 특수문자나 이모지, 숫자 등을 제거해봅시다.\n","  - <mark>한글 문자(초성 제외), 영어 대문자, 영어 소문자, 띄어쓰기 이외의 문자를 제외</mark>하는 정규표현식 작성\n","2. 영어 단어의 경우 같은 단어들이 같은 토큰으로 분류될 수 있도록 <mark>대문자로 통일</mark>해줍니다."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:21:07.164784Z","iopub.status.busy":"2023-06-02T18:21:07.163814Z","iopub.status.idle":"2023-06-02T18:21:07.890959Z","shell.execute_reply":"2023-06-02T18:21:07.890352Z","shell.execute_reply.started":"2023-06-02T18:21:07.164734Z"},"papermill":{"duration":0.982216,"end_time":"2023-05-08T06:10:57.296264","exception":false,"start_time":"2023-05-08T06:10:56.314048","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/2785118696.py:23: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  x_train_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_train.iteritems(), total=len(x_train), desc=\"pre-processing data\")]\n","pre-processing data: 100%|██████████| 149993/149993 [00:00<00:00, 212474.81it/s]\n"]}],"source":["##########################################################################################\n","# Empty Module #1\n","# 입력: 자연어 상태의 리뷰 텍스트\n","# 출력: 한글(초성 제외), 영어 대문자, 띄어쓰기로만 구성된 텍스트 \n","# 입력 예시: \"안녕 Hello!!!:)ㅎㅎ반갑다.\"\n","# 출력 예시: \"안녕 HELLO반갑다\"\n","##########################################################################################\n","import re\n","\n","# 정규표현식 코드, 한글문자(ㄱ-ㅎ,ㅏ-ㅣ,가-힣),영어대문자(A-Z), 영어소문자(a-z), 띄어쓰기(\\s)\n","pattern = '[^ㄱ-ㅎㅏ-ㅣ가-힣A-Za-z\\s]'\n","\n","def apply_regex(pattern, text):  # 정규표현식을 이용한 필터링 적용\n","    \n","    #한글문자(ㄱ-ㅎ,ㅏ-ㅣ,가-힣),영어대문자(A-Z), 영어소문자(a-z), 띄어쓰기(\\s)를 제외하고 다른문자는 전부 삭제\n","    text = re.sub(pattern, \"\", text)  # 정규표현식 패턴에 맞는 값들을 텍스트에서 제거 \n","    \n","    # 영어들을 찾아 대문자로 치환하는 코드\n","    text = text.upper()\n","    text = re.sub(\"[A-Z]+\", lambda m: m.group().upper(), text)\n","    return text\n","\n","x_train_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_train.iteritems(), total=len(x_train), desc=\"pre-processing data\")]"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.014265,"end_time":"2023-05-08T06:10:57.323679","exception":false,"start_time":"2023-05-08T06:10:57.309414","status":"completed"},"tags":[]},"source":["## \\[Empty Module #2\\] 단어 토큰화\n","### Open Korean Text(OKT)를 이용한 단어 토큰화(Tokenization)\n","- 한국어 자연어 처리 라이브러리인 konlpy의 OKT 래퍼를 통하여 문장을 단어로 토큰화해봅시다.\n","- 아래 도큐먼트를 참고하여 토큰화를 수행합니다.\n","  - <mark>이때, OKT 클래스의 특정 매개변수를 이용해 어근화를 진행해줍니다. (도큐먼트 참고)</mark>\n","- konlpy 도큐먼트: https://konlpy.org/ko/latest/api/konlpy.tag/#okt-class\n","- OKT 도큐먼트: https://github.com/open-korean-text/open-korean-text"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:21:07.891950Z","iopub.status.busy":"2023-06-02T18:21:07.891723Z","iopub.status.idle":"2023-06-02T18:21:09.143904Z","shell.execute_reply":"2023-06-02T18:21:09.142961Z","shell.execute_reply.started":"2023-06-02T18:21:07.891929Z"},"papermill":{"duration":1.373034,"end_time":"2023-05-08T06:10:58.70747","exception":false,"start_time":"2023-05-08T06:10:57.334436","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["##########################################################################################\n","# Empty Module #2\n","# 입력: 자연어 상태의 리뷰 데이터\n","# 출력: 토큰화와 과정을 거쳐 단어들의 리스트로 변환된 데이터\n","# 입력 예시: \"커피는 역시 학생회관 커피\"\n","# 출력 예시: [\"커피\", \"는\", \"역시\", \"학생\", \"회관\", \"커피\"]\n","##########################################################################################\n","from konlpy.tag import Okt\n","okt = Okt()\n","\n","def tokenize_words(sentence):\n","    # 한국어 텍스트를 형태소 단위로 토큰화하는 작업을 수행, stem=True 파라미터는 형태소 분석 과정에서 어간 추출을 수행\n","    sentence_tokenized = okt.morphs(sentence, stem=True)# 여기에 코드 작성\n","    return sentence_tokenized"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:21:09.145478Z","iopub.status.busy":"2023-06-02T18:21:09.145132Z","iopub.status.idle":"2023-06-02T18:28:00.872911Z","shell.execute_reply":"2023-06-02T18:28:00.872147Z","shell.execute_reply.started":"2023-06-02T18:21:09.145448Z"},"papermill":{"duration":573.138395,"end_time":"2023-05-08T06:20:31.856663","exception":false,"start_time":"2023-05-08T06:10:58.718268","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["tokenizing data: 100%|██████████| 149993/149993 [06:51<00:00, 364.31it/s]\n"]}],"source":["# 약 10-15분 정도 소요됩니다. \n","x_train_tokenized = [tokenize_words(x) for x in tqdm(x_train_preprocessed, desc=\"tokenizing data\")]"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.226231,"end_time":"2023-05-08T06:20:32.313486","exception":false,"start_time":"2023-05-08T06:20:32.087255","status":"completed"},"tags":[]},"source":["## \\[Empty Module #3\\] 불용어 제거\n","- 조사를 비롯한 불용어들은 많은 횟수 등장하지만, 리뷰의 긍정과 부정 여부를 판단하는데는 도움이 되지 않습니다.\n","- 데이터에서 아래 리스트로 정의된 불용어들을 제거해줍니다."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:28:00.874745Z","iopub.status.busy":"2023-06-02T18:28:00.874228Z","iopub.status.idle":"2023-06-02T18:28:01.866447Z","shell.execute_reply":"2023-06-02T18:28:01.864757Z","shell.execute_reply.started":"2023-06-02T18:28:00.874717Z"},"papermill":{"duration":1.425199,"end_time":"2023-05-08T06:20:33.964564","exception":false,"start_time":"2023-05-08T06:20:32.539365","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']  #별다른 의미가 없는 불용어들\n","\n","def exclude_stopwords(text):\n","    # 위 리스트에 포함된 불용어들을 제거하는 코드 작성\n","    tokenized_data = [word for word in text if word not in stopwords]\n","    return tokenized_data\n","\n","x_train_stopwords_excluded = [exclude_stopwords(x) for x in x_train_tokenized]"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.227527,"end_time":"2023-05-08T06:20:34.416544","exception":false,"start_time":"2023-05-08T06:20:34.189017","status":"completed"},"tags":[]},"source":["## \\[Empty Module #4\\] 단어 임베딩\n","### 단어 임베딩 코드 구현\n","- 토큰화를 거쳐 분리된 단어들을 하나의 정수 값으로 매핑해주는 희소 표현법을 직접 구현해봅시다.\n","- 입력된 단어가 새로운 단어라면 새로운 정수 값을 할당하고, 이전에 등장한 단어라면 이전에 할당한 정수를 할당하는 함수를 작성합니다.\n","  - 단, <mark>테스트 데이터에 대해서는 새로운 단어가 등장하면 값을 할당하지 않습니다.</mark>"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:28:01.870451Z","iopub.status.busy":"2023-06-02T18:28:01.870012Z","iopub.status.idle":"2023-06-02T18:28:01.879733Z","shell.execute_reply":"2023-06-02T18:28:01.878111Z","shell.execute_reply.started":"2023-06-02T18:28:01.870419Z"},"papermill":{"duration":0.236545,"end_time":"2023-05-08T06:20:34.876645","exception":false,"start_time":"2023-05-08T06:20:34.6401","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["##########################################################################################\n","# Empty Module #4\n","# 입력: 단어 토큰화된 데이터\n","# 출력: 임베딩 과정을 거쳐, 각 단어가 하나의 실수 값으로 표현된 데이터\n","# 입력 예시: [\"커피\", \"역시\", \"학생\", \"회관\", \"커피\"]\n","# 출력 예시: [0, 1, 2, 3, 0]\n","##########################################################################################\n","\n","embedding_dict = dict()  # 단어 임베딩을 위한 딕셔너리\n","embedding_value = 0\n","\n","def embed_tokens(sentence_tokenized, mode):\n","    assert mode.upper() in [\"TRAIN\", \"TEST\"]\n","    global embedding_value\n","    \n","    sentence_embedded = list()\n","    for word in sentence_tokenized:\n","        # 코드 작성\n","        # mode가 \"TRAIN\"인 경우 \"embedding_dict\"에 새로운 단어를 추가하고 그에 대응하는 임베딩 값을 할당하는 작업을 수행\n","        # mode가 \"TEST\"인 경우 \"embedding_dict\"에 있는 단어들에 대응하는 임베딩 값을 \"sentence_embedded\" 리스트에 추가하는 작업을 수행\n","        \n","        if mode.upper() == \"TRAIN\": # mode가 \"TRAIN\"과 일치하는 경우:\n","            if word not in embedding_dict: # \"word\"가 \"embedding_dict\"에 없는 경우:\n","                embedding_dict[word] = embedding_value # \"embedding_dict\"에 \"word\"를 추가하고, \"embedding_value\" 값을 할당.\n","                embedding_value += 1 # \"embedding_value\"를 1 증가.\n","            sentence_embedded.append(embedding_dict[word]) # \"sentence_embedded\" 리스트에 \"embedding_dict[word]\"를 추가.\n","        else:  # mode가 \"TEST\"와 일치하는 경우:\n","            if word in embedding_dict: # \"word\"가 \"embedding_dict\"에 있는 경우:\n","                sentence_embedded.append(embedding_dict[word]) #\"sentence_embedded\" 리스트에 \"embedding_dict[word]\"를 추가.\n","    return sentence_embedded"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:28:01.881719Z","iopub.status.busy":"2023-06-02T18:28:01.881329Z","iopub.status.idle":"2023-06-02T18:28:03.002845Z","shell.execute_reply":"2023-06-02T18:28:03.002149Z","shell.execute_reply.started":"2023-06-02T18:28:01.881686Z"},"papermill":{"duration":1.328669,"end_time":"2023-05-08T06:20:36.428533","exception":false,"start_time":"2023-05-08T06:20:35.099864","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["embedding data: 100%|██████████| 149993/149993 [00:01<00:00, 136682.51it/s]"]},{"name":"stdout","output_type":"stream","text":["[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21], [22, 4, 15]]\n","총 46058개의 단어가 임베딩되었습니다.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# 실행 시간이 제법 소요됩니다. 비정상이 아니니 걱정하지 않으셔도 됩니다.\n","x_train_embedded = [embed_tokens(x, mode=\"TRAIN\") for x in tqdm(x_train_stopwords_excluded, desc=\"embedding data\")]\n","print(x_train_embedded[:5])\n","print(\"총 %d개의 단어가 임베딩되었습니다.\"%(embedding_value))"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.284019,"end_time":"2023-05-08T06:20:36.936606","exception":false,"start_time":"2023-05-08T06:20:36.652587","status":"completed"},"tags":[]},"source":["## \\[Empty Module #5\\] 문장 벡터화\n","### Bag of Words 방법을 사용한 문장 벡터화\n","- 캐글 프로젝트 설명 페이지의 Bag of Words 방법 설명을 참고하여 Bag of Words 방법을 직접 구현해봅시다."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:28:03.004064Z","iopub.status.busy":"2023-06-02T18:28:03.003802Z","iopub.status.idle":"2023-06-02T18:28:07.636416Z","shell.execute_reply":"2023-06-02T18:28:07.635224Z","shell.execute_reply.started":"2023-06-02T18:28:03.004039Z"},"papermill":{"duration":7.419859,"end_time":"2023-05-08T06:20:44.579436","exception":false,"start_time":"2023-05-08T06:20:37.159577","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["making BoW representation: 100%|██████████| 149993/149993 [00:04<00:00, 32464.52it/s]\n"]}],"source":["##########################################################################################\n","# Empty Module #5\n","# 입력: 임베딩 과정을 거친 데이터\n","# 출력: BoW 형태로 변환되어, M차원의 고정된 크기를 가진 벡터로 변환된 데이터\n","# 힌트: np.zeros((2, 3))는 [2, 3] 크기의 0으로 가득 찬 행렬을 생성합니다.\n","##########################################################################################\n","M = len(embedding_dict) # 전체 단어의 수\n","\n","# Bag of Words(BoW) 방법을 사용하여 모든 문장을 동일한 길이의 벡터로 만들어 줄 수 있습니다.\n","# to_BoW_representation 함수를 사용하여 주어진 문장들을 Bag-of-Words (BoW) 표현으로 변환하는 작업을 수행\n","def to_BoW_representation(x): #문장들의 임베딩 표현인 \"x\"를 입력으로 받음\n","    # BoW는 어떤 shape를 가져야 할까요? \n","    # ANS)\n","    # x의 길이(len(x))는 문장의 개수를 나타냄\n","    # M은 embedding_dict에 저장된 단어의 총 개수\n","    # x_BoW는 문장들의 Bag-of-Words (BoW) 표현을 저장하는 배열이며, 각 문장의 BoW 표현은 단어의 개수를 나타내는 M차원 벡터로 표현\n","    # x_BoW의 크기는 (len(x), M)이므로, \"len(x)\" 개수의 문장에 대한 BoW 표현을 저장하기 위해 2차원 배열로 생성\n","    # x_BoW[i]는 x의 i번째 문장에 대한 BoW 표현을 나타내는 벡터. 이 벡터는 embedding_dict에 저장된 단어의 총 개수 M만큼의 길이를 가지며\n","    # 각 인덱스는 해당 단어의 등장 횟수를 나타냄\n","    shape = (len(x), M)\n","    \n","    x_BoW = np.zeros(shape)\n","    for i in tqdm(range(len(x)), desc=\"making BoW representation\"):\n","        # 여기에 BoW 구현\n","        # np.unique(x[i], return_counts=True)을 사용하여 문장 내의 고유한 단어와 그 등장 횟수를 가져옴\n","        # unique에는 고유한 단어가, counts에는 해당 단어의 등장 횟수가 저장됨\n","        # unique 배열의 요소들을 정수로 변환한 후, 해당 인덱스에 대응하는 x_BoW[i] 의 값을 counts로 설정\n","        # 모든 문장에 대한 BoW 표현이 완성된 x_BoW를 반환\n","        \n","        unique, counts = np.unique(x[i], return_counts=True)\n","        unique_int = unique.astype(int)\n","        x_BoW[i][unique_int] = counts\n","        \n","    return x_BoW\n","\n","x_train_BoW = to_BoW_representation(x_train_embedded)"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.226259,"end_time":"2023-05-08T06:20:45.030446","exception":false,"start_time":"2023-05-08T06:20:44.804187","status":"completed"},"tags":[]},"source":["## \\[Empty Module #6\\] 차원 축소\n","- 우리가 만든 BoW는 수많은 리뷰에 등장하는 모든 단어들을 사용하여 만들어졌기 때문에, 엄청난 양의 단어들을 가지고 있습니다.\n","- 그러나, 실제로 영화 리뷰에 쓰이는 단어들은 이보다 적기 때문에, 많은 단어들이 전체 데이터에서 실제로는 한번도 등장하지 않거나, 매우 조금 등장하면서 공간을 차지하고 있을 것 입니다.\n","- 데이터의 크기를 줄여 머신러닝 모델이 중요한 정보에 집중할 수 있도록 해봅시다.\n","- <mark>학습 데이터에서 50번 미만으로 등장한 단어들을 제외</mark>해줍니다."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:28:07.637481Z","iopub.status.busy":"2023-06-02T18:28:07.637269Z","iopub.status.idle":"2023-06-02T18:28:07.642311Z","shell.execute_reply":"2023-06-02T18:28:07.641263Z","shell.execute_reply.started":"2023-06-02T18:28:07.637462Z"},"papermill":{"duration":0.236099,"end_time":"2023-05-08T06:20:45.493465","exception":false,"start_time":"2023-05-08T06:20:45.257366","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["##########################################################################################\n","# Empty Module #6\n","# 입력: BoW 형태로 변환된 (N, M) 크기의 데이터\n","# 출력: 등장 빈도가 적은 단어들을 제외한 (N, m) 크기의 더 작은 데이터\n","##########################################################################################"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:28:07.644137Z","iopub.status.busy":"2023-06-02T18:28:07.643570Z","iopub.status.idle":"2023-06-02T18:28:07.655098Z","shell.execute_reply":"2023-06-02T18:28:07.653929Z","shell.execute_reply.started":"2023-06-02T18:28:07.644114Z"},"papermill":{"duration":7.501002,"end_time":"2023-05-08T06:20:53.222732","exception":false,"start_time":"2023-05-08T06:20:45.72173","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# 힌트\n","# 1. 먼저 전체 데이터에서 각 단어가 등장한 횟수를 세어보세요.\n","# 2. 그 다음, 등장 횟수가 50회 미만인 단어들을 찾습니다.\n","# 3. 해당 단어들을 데이터에서 제거하는 코드를 작성합니다.\n","# 4. 설계를 잘 하고 구현을 시작해야 어렵지 않습니다."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:28:07.657164Z","iopub.status.busy":"2023-06-02T18:28:07.656785Z","iopub.status.idle":"2023-06-02T18:28:22.677978Z","shell.execute_reply":"2023-06-02T18:28:22.676861Z","shell.execute_reply.started":"2023-06-02T18:28:07.657131Z"},"papermill":{"duration":12.114323,"end_time":"2023-05-08T06:21:05.627314","exception":false,"start_time":"2023-05-08T06:20:53.512991","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["원본 BoW 크기: (149993, 46058)\n","차원 축소 후 크기: (149993, 3172)\n"]}],"source":["# 여기에 코드 작성\n","# BoW 표현에서 빈도가 임계값(threshold)보다 낮은 단어들을 제외하는 작업을 수행\n","# 임계값(threshold) : 50\n","threshold = 50\n","\n","def exclude_low_frequency_words(x, threshold): # BoW 형태로 변환된 (N, M) 크기의 데이터인 x와 임계값인 threshold를 입력\n","    word_counts = np.sum(x, axis=0) # x의 각 열(단어)에 대해 총 빈도를 계산하여 word_counts 배열에 저장, 이는 각 단어의 전체 등장 횟수를 나타냄\n","    indices = np.where(word_counts >= threshold)[0] # word_counts 배열에서 임계값보다 크거나 같은 값을 가지는 인덱스를 추출. 이를 indices배열에 저장.\n","    return indices # 인덱스를 추출\n","\n","indices = exclude_low_frequency_words(x_train_BoW, threshold) #  x_train_BoW 에서 빈도가 임계값보다 낮은 단어들을 제외\n","x_train_BoW_reduced = x_train_BoW[:, indices] # 그에 해당하는 열(단어)을 선택하여 x_train_BoW_reduced를 생성\n","\n","print(\"원본 BoW 크기:\", x_train_BoW.shape) \n","print(\"차원 축소 후 크기:\", x_train_BoW_reduced.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.22558,"end_time":"2023-05-08T06:21:06.079455","exception":false,"start_time":"2023-05-08T06:21:05.853875","status":"completed"},"tags":[]},"source":["## \\[Empty Module #7\\]  분류 수행 및 제출: Bag of Words\n","- 이제 모든 문장이 고정된 크기 $m$ 차원의 벡터로 변환되었습니다.\n","- 원하는 모델을 사용하여, 각 문장의 영화에 대한 긍정적인 리뷰인지, 부정적인 리뷰인지 분류해봅시다.(Baseline 모델은 로지스틱 회귀입니다)\n","- 그 다음, 결과를 기록하여 Kaggle에 제출해봅시다!"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:28:22.679508Z","iopub.status.busy":"2023-06-02T18:28:22.679153Z","iopub.status.idle":"2023-06-02T18:28:22.686554Z","shell.execute_reply":"2023-06-02T18:28:22.685264Z","shell.execute_reply.started":"2023-06-02T18:28:22.679477Z"},"papermill":{"duration":43.097338,"end_time":"2023-05-08T06:21:49.406392","exception":false,"start_time":"2023-05-08T06:21:06.309054","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["##########################################################################################\n","# Empty Module #7\n","# 지금까지 전처리한 데이터로 분류를 수행하여, kaggle에 제출해봅시다.\n","# Baseline은 로지스틱 회귀입니다.\n","##########################################################################################\n","# 분류기 정의 및 학습 수행 코드 작성"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:28:22.688075Z","iopub.status.busy":"2023-06-02T18:28:22.687764Z","iopub.status.idle":"2023-06-02T18:31:46.551653Z","shell.execute_reply":"2023-06-02T18:31:46.550317Z","shell.execute_reply.started":"2023-06-02T18:28:22.688048Z"},"papermill":{"duration":275.661355,"end_time":"2023-05-08T06:26:28.190092","exception":false,"start_time":"2023-05-08T06:21:52.528737","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/1223158976.py:3: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  x_test_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_test.iteritems(), total=len(x_test), desc=\"pre-processing data\")]\n","pre-processing data: 100%|██████████| 49999/49999 [00:00<00:00, 201709.98it/s]\n","tokenizing data: 100%|██████████| 49999/49999 [03:18<00:00, 251.59it/s]\n","embedding data: 100%|██████████| 49999/49999 [00:00<00:00, 151739.20it/s]\n","making BoW representation: 100%|██████████| 49999/49999 [00:01<00:00, 31077.62it/s]\n"]}],"source":["# TEST 데이터를 전처리\n","x_test = test_data[\"review\"]\n","x_test_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_test.iteritems(), total=len(x_test), desc=\"pre-processing data\")]\n","x_test_tokenized = [tokenize_words(x) for x in tqdm(x_test_preprocessed, desc=\"tokenizing data\")]\n","x_test_stopwords_excluded = [exclude_stopwords(x) for x in x_test_tokenized]\n","x_test_embedded = [embed_tokens(x, mode=\"TEST\") for x in tqdm(x_test_stopwords_excluded, desc=\"embedding data\")]\n","x_test_BoW = to_BoW_representation(x_test_embedded)\n","x_test_BoW_reduced = x_test_BoW[:, indices]"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:31:46.554467Z","iopub.status.busy":"2023-06-02T18:31:46.553726Z","iopub.status.idle":"2023-06-02T18:32:20.113575Z","shell.execute_reply":"2023-06-02T18:32:20.112738Z","shell.execute_reply.started":"2023-06-02T18:31:46.554438Z"},"papermill":{"duration":1.030924,"end_time":"2023-05-08T06:26:29.528144","exception":false,"start_time":"2023-05-08T06:26:28.49722","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stdout","output_type":"stream","text":["0.889928196649177\n"]}],"source":["# TEST 데이터에 대한 예측 수행 코드 작성\n","# BoW vs TF-IDF 비교결과 BoW의 성능이 더 높으므로 BoW 선택\n","\n","# 이진분류를 위해 로지스틱 회귀를 사용한다\n","from sklearn.linear_model import LogisticRegression\n","lgt = LogisticRegression(class_weight='balanced', max_iter = 1000)\n","\n","\n","lgt.fit(x_train_BoW, y_train)\n","# 차원축소 된 데이터로 학습시 score = 0.8430793437\n","# 차원축소가 되지않은 데이터로 학습시 score = 0.889928196\n","# 차원축소가 되지않은 데이터로 학습시 판별능력이 상승하는 걸 확인할 수 있다. 따라서 차원축소가 되지않은 train 모델을 사용한다\n","print(lgt.score(x_train_BoW, y_train))\n","\n","pred = lgt.predict(x_test_BoW)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:32:20.120919Z","iopub.status.busy":"2023-06-02T18:32:20.118315Z","iopub.status.idle":"2023-06-02T18:32:20.233729Z","shell.execute_reply":"2023-06-02T18:32:20.232563Z","shell.execute_reply.started":"2023-06-02T18:32:20.120856Z"},"papermill":{"duration":0.489598,"end_time":"2023-05-08T06:26:30.391147","exception":false,"start_time":"2023-05-08T06:26:29.901549","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# TEST 데이터에 대한 예측 값을 csv로 저장하는 코드 작성\n","submit = pd.read_csv(\"/kaggle/input/2023-ml-project1/sample_submission.csv\", index_col=0)\n","\n","submit['id'] = submit.index\n","submit['rating'] = pred.astype('int64')\n","\n","submit = submit[['id', 'rating']]\n","submit.to_csv(\"submit.csv\", header=True, index=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.301876,"end_time":"2023-05-08T06:26:30.999594","exception":false,"start_time":"2023-05-08T06:26:30.697718","status":"completed"},"tags":[]},"source":["## \\[Empty Module #8\\] TF-IDF 적용\n","- BoW에서는 고려하지 않는 각 단어들의 중요도를 고려하기 위해, TF-IDF를 적용해봅시다.\n","- 캐글 프로젝트 설명 페이지의 설명을 참고하여 빈칸을 채워, TF-IDF를 구현해봅시다."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:32:20.235275Z","iopub.status.busy":"2023-06-02T18:32:20.234948Z","iopub.status.idle":"2023-06-02T18:32:24.329211Z","shell.execute_reply":"2023-06-02T18:32:24.328214Z","shell.execute_reply.started":"2023-06-02T18:32:20.235240Z"},"papermill":{"duration":0.995899,"end_time":"2023-05-08T06:26:32.302887","exception":false,"start_time":"2023-05-08T06:26:31.306988","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","##########################################################################################\n","# Empty Module #8\n","# 빈칸을 적절히 채워넣어 TF-IDF를 위한 Inverse Document Frequency를 계산해봅시다.\n","##########################################################################################\n","N = x_train_BoW.shape[0]  # 총 데이터 샘플의 수\n","\n","# x에 대해 각 단어의 문서 빈도를 계산하여 document_frequency 배열에 저장. 이는 각 단어가 등장한 문서의 수를 나타냄\n","def calculate_document_frequency(x):\n","    document_frequency = np.sum(x > 0, axis=1)\n","    return document_frequency\n","\n","# inverse document frequency값은 document frequency에 반비례\n","# document_frequency 배열을 입력으로 받아 각 단어의 inverse document frequency (IDF) 값을 계산하여 inverse_document_frequency 배열에 저장\n","# IDF는 전체 문서 수에 대한 해당 단어가 등장한 문서의 비율에 대한 로그값\n","def calculate_inverse_document_frequency(document_frequency, N):\n","    inverse_document_frequency = np.log(N / (document_frequency + 1))\n","    return inverse_document_frequency\n","\n","# x_train_BoW 에 대한 document_frequency와 inverse document frequency 를 구하기\n","document_frequency = calculate_document_frequency(x_train_BoW)\n","inverse_document_frequency = calculate_inverse_document_frequency(document_frequency, N)\n","inverse_document_frequency = inverse_document_frequency.reshape(-1, 1)\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:32:24.330509Z","iopub.status.busy":"2023-06-02T18:32:24.330256Z","iopub.status.idle":"2023-06-02T18:32:24.935034Z","shell.execute_reply":"2023-06-02T18:32:24.933535Z","shell.execute_reply.started":"2023-06-02T18:32:24.330486Z"},"papermill":{"duration":1.56492,"end_time":"2023-05-08T06:26:34.1783","exception":false,"start_time":"2023-05-08T06:26:32.61338","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# 차원 축소된 x_train_BoW인 x_train_BoW_reduced를 인자로 받아 문서 빈도와 IDF 값을 곱하여 TF-IDF 값을 계산\n","x_train_tfidf = x_train_BoW_reduced * inverse_document_frequency"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.302103,"end_time":"2023-05-08T06:26:34.784772","exception":false,"start_time":"2023-05-08T06:26:34.482669","status":"completed"},"tags":[]},"source":["## \\[Empty Module #9\\]  분류 수행 및 제출: TF-IDF\n","- TF-IDF를 적용한 결과를 기록하여 Kaggle에 제출해봅시다!"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:32:24.936682Z","iopub.status.busy":"2023-06-02T18:32:24.936358Z","iopub.status.idle":"2023-06-02T18:32:24.940796Z","shell.execute_reply":"2023-06-02T18:32:24.939989Z","shell.execute_reply.started":"2023-06-02T18:32:24.936654Z"},"papermill":{"duration":50.621027,"end_time":"2023-05-08T06:27:25.711221","exception":false,"start_time":"2023-05-08T06:26:35.090194","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["##########################################################################################\n","# Empty Module #9\n","# TEST 데이터에 TF-IDF를 적용하여 모델을 학습, 예측을 수행하고 kaggle에 제출해봅시다.\n","# 이때, BoW와 같은 모델을 사용하여 성능을 비교해봅시다.\n","##########################################################################################\n","# 분류기 정의 및 학습 수행 코드 작성"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:32:24.942592Z","iopub.status.busy":"2023-06-02T18:32:24.942275Z","iopub.status.idle":"2023-06-02T18:32:26.426920Z","shell.execute_reply":"2023-06-02T18:32:26.425358Z","shell.execute_reply.started":"2023-06-02T18:32:24.942563Z"},"papermill":{"duration":1.234079,"end_time":"2023-05-08T06:27:28.588531","exception":false,"start_time":"2023-05-08T06:27:27.354452","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","# TEST 데이터를 전처리하는 코드 작성\n","\n","# TEST 데이터에 대한 document frequency 구하기\n","# TEST 데이터에 대한 IDF 구하기\n","N_t = x_test_BoW.shape[0]\n","document_frequency = calculate_document_frequency(x_test_BoW)\n","inverse_document_frequency = calculate_inverse_document_frequency(document_frequency, N_t)\n","inverse_document_frequency = inverse_document_frequency.reshape(-1, 1)\n","\n","# TEST 데이터에 대한 문서 빈도와 IDF 값을 곱하여 TF-IDF 값을 계산\n","x_test_tfidf = x_test_BoW_reduced * inverse_document_frequency\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:32:26.428970Z","iopub.status.busy":"2023-06-02T18:32:26.428570Z","iopub.status.idle":"2023-06-02T18:32:26.437225Z","shell.execute_reply":"2023-06-02T18:32:26.436003Z","shell.execute_reply.started":"2023-06-02T18:32:26.428933Z"},"papermill":{"duration":0.569811,"end_time":"2023-05-08T06:27:29.46472","exception":false,"start_time":"2023-05-08T06:27:28.894909","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["'\\n# 예측을 수행하는 코드 작성\\nfrom sklearn.linear_model import LogisticRegression\\nlgt = LogisticRegression(max_iter = 1000)\\nlgt.fit(x_train_tfidf, y_train)\\nprint(lgt.score(x_train_tfidf, y_train))\\n\\npred = lgt.predict(x_test_tfidf)\\n'"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","# 예측을 수행하는 코드 작성\n","from sklearn.linear_model import LogisticRegression\n","lgt = LogisticRegression(max_iter = 1000)\n","lgt.fit(x_train_tfidf, y_train)\n","print(lgt.score(x_train_tfidf, y_train))\n","\n","pred = lgt.predict(x_test_tfidf)\n","'''"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:32:26.438610Z","iopub.status.busy":"2023-06-02T18:32:26.438351Z","iopub.status.idle":"2023-06-02T18:32:26.452158Z","shell.execute_reply":"2023-06-02T18:32:26.451029Z","shell.execute_reply.started":"2023-06-02T18:32:26.438586Z"},"papermill":{"duration":0.401319,"end_time":"2023-05-08T06:27:30.302375","exception":false,"start_time":"2023-05-08T06:27:29.901056","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["'\\nsubmit = pd.read_csv(\"/kaggle/input/2023-ml-project1/sample_submission.csv\", index_col=0)\\n#TEST 데이터에 대한 예측 값을 csv로 저장하는 코드 작성\\nsubmit[\\'id\\'] = submit.index\\nsubmit[\\'rating\\'] = pred.astype(\\'int64\\')\\n\\nsubmit = submit[[\\'id\\', \\'rating\\']]\\nprint(submit.head())\\nsubmit.to_csv(\"submit.csv\", header=True, index=False)\\n'"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","submit = pd.read_csv(\"/kaggle/input/2023-ml-project1/sample_submission.csv\", index_col=0)\n","#TEST 데이터에 대한 예측 값을 csv로 저장하는 코드 작성\n","submit['id'] = submit.index\n","submit['rating'] = pred.astype('int64')\n","\n","submit = submit[['id', 'rating']]\n","print(submit.head())\n","submit.to_csv(\"submit.csv\", header=True, index=False)\n","'''"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","## 결과 비교 TIP\n","- BoW와 TF-IDF에 같은 모델을 적용하여 성능의 차이를 비교해봅시다.\n","- 각각의 방법론에 여러가지 모델을 적용하며, 사용한 방법에 따라 더 적절한 모델이 있는지 고려해봅시다.\n","- 실험 결과들을 토대로, 왜 이런 결과가 도출되었는지 고민해봅시다."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
